import aiohttp
import asyncio
import csv
import re
import time
import yaml
from pathlib import Path
from datetime import datetime
from transformers import pipeline

# ========== åŠ è½½é…ç½® ==========
CONFIG_PATH = Path("config.yml")
if CONFIG_PATH.exists():
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
else:
    config = {"keywords": ["sale", "discount", "promotion", "offer", "clearance"],
              "notify": False,
              "output": "results.csv"}

# ========== è¯»å–ç½‘ç«™åˆ—è¡¨ ==========
SITES_PATH = Path("sites.csv")
if not SITES_PATH.exists():
    print("âš ï¸ æœªæ‰¾åˆ° sites.csvï¼Œè¯·å…ˆåˆ›å»ºè¯¥æ–‡ä»¶ã€‚")
    exit()

with open(SITES_PATH, "r", encoding="utf-8") as f:
    reader = csv.reader(f)
    sites = [row[0].strip() for row in reader if row]

# ========== åˆå§‹åŒ–AIæ¨¡å‹ ==========
print("ğŸ§  æ­£åœ¨åŠ è½½AIæ¨¡å‹ï¼ˆé¦–æ¬¡åŠ è½½å¯èƒ½è¾ƒæ…¢ï¼‰...")
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")

# ========== å¼‚æ­¥æŠ“å–ç½‘é¡µ ==========
async def fetch(session, url):
    try:
        async with session.get(url, timeout=15) as resp:
            if resp.status == 200:
                text = await resp.text(errors="ignore")
                return url, text
            else:
                return url, f"Error: {resp.status}"
    except Exception as e:
        return url, f"Exception: {str(e)}"

async def crawl_all():
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url) for url in sites]
        results = await asyncio.gather(*tasks)
        return results

# ========== å…³é”®è¯ + AI åˆ¤æ–­ ==========
def check_promotions(text, keywords):
    text_lower = text.lower()
    if any(k.lower() in text_lower for k in keywords):
        return True

    # æå–ç½‘é¡µå‰1000å­—ç¬¦
    snippet = text[:1000]
    prediction = classifier(snippet)[0]
    if prediction["label"] == "POSITIVE" and prediction["score"] > 0.9:
        return True
    return False

# ========== ä¸»ç¨‹åº ==========
async def main():
    print(f"ğŸš€ å¼€å§‹æ£€æµ‹ï¼Œå…± {len(sites)} ä¸ªç½‘ç«™...")
    results = await crawl_all()
    detected = []

    for url, content in results:
        if "Error" in content or "Exception" in content:
            status = "âŒ è®¿é—®å¤±è´¥"
        else:
            has_promo = check_promotions(content, config.get("keywords", []))
            status = "âœ… æœ‰ä¿ƒé”€" if has_promo else "â€” æœªå‘ç°ä¿ƒé”€"
        detected.append([datetime.now().strftime("%Y-%m-%d %H:%M:%S"), url, status])
        print(f"{url} â†’ {status}")

    # å†™å…¥CSV
    output_path = Path(config.get("output", "results.csv"))
    with open(output_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["æ—¶é—´", "ç½‘å€", "æ£€æµ‹ç»“æœ"])
        writer.writerows(detected)

    print(f"\nğŸ“ ç»“æœå·²å†™å…¥ {output_path.resolve()}")

if __name__ == "__main__":
    asyncio.run(main())
